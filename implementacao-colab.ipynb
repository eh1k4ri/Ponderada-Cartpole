{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eh1k4ri/Ponderada-Cartpole/blob/main/implementacao-colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yJoTLS2Xel9d",
        "outputId": "421f0c24-b0c4-495c-e64f-05d90db36392"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: gym[classic_control] in /home/esthernunes/.local/lib/python3.10/site-packages (0.26.2)\n",
            "Requirement already satisfied: matplotlib in /home/esthernunes/.local/lib/python3.10/site-packages (3.9.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /home/esthernunes/.local/lib/python3.10/site-packages (from gym[classic_control]) (3.1.0)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /home/esthernunes/.local/lib/python3.10/site-packages (from gym[classic_control]) (1.26.4)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /home/esthernunes/.local/lib/python3.10/site-packages (from gym[classic_control]) (0.0.8)\n",
            "Requirement already satisfied: pygame==2.1.0 in /home/esthernunes/.local/lib/python3.10/site-packages (from gym[classic_control]) (2.1.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /home/esthernunes/.local/lib/python3.10/site-packages (from matplotlib) (1.3.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/lib/python3/dist-packages (from matplotlib) (9.0.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/lib/python3/dist-packages (from matplotlib) (2.4.7)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /home/esthernunes/.local/lib/python3.10/site-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /home/esthernunes/.local/lib/python3.10/site-packages (from matplotlib) (4.54.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /home/esthernunes/.local/lib/python3.10/site-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /home/esthernunes/.local/lib/python3.10/site-packages (from matplotlib) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /home/esthernunes/.local/lib/python3.10/site-packages (from matplotlib) (24.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "# Importar bibliotecas necessárias\n",
        "%pip install gym[classic_control] matplotlib\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import gym"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XByOBPY1el9h"
      },
      "outputs": [],
      "source": [
        "# Configuração do Ambiente\n",
        "env = gym.make('CartPole-v1', render_mode='human')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K_yQTamIel9h"
      },
      "outputs": [],
      "source": [
        "# Parâmetros de discretização\n",
        "n_bins = 24  # Número de divisões em cada dimensão do estado\n",
        "bins = [\n",
        "    np.linspace(-4.8, 4.8, n_bins),        # Posição do carrinho\n",
        "    np.linspace(-5, 5, n_bins),            # Velocidade do carrinho\n",
        "    np.linspace(-0.418, 0.418, n_bins),    # Ângulo do pêndulo\n",
        "    np.linspace(-5, 5, n_bins)             # Velocidade angular do pêndulo\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dhusj7-Sel9i"
      },
      "outputs": [],
      "source": [
        "# Função para discretizar estados contínuos\n",
        "def discretize_state(state):\n",
        "    state_index = []\n",
        "    for i, val in enumerate(state):\n",
        "        state_index.append(np.digitize(val, bins[i]) - 1)\n",
        "    return tuple(state_index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pv_kv2qXel9j"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Inicializar a tabela Q com valores aleatórios\n",
        "action_space_size = env.action_space.n  # Número de ações possíveis (2: esquerda ou direita)\n",
        "state_space_size = [n_bins] * len(bins)  # Dimensões do espaço de estados discretizado\n",
        "Q_table = np.random.uniform(low=-1, high=1, size=state_space_size + [action_space_size])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9mdthZqlel9k"
      },
      "outputs": [],
      "source": [
        "# Definição dos hiperparâmetros\n",
        "alpha = 0.1          # Taxa de aprendizado\n",
        "gamma = 0.99         # Fator de desconto\n",
        "epsilon = 1.0        # Taxa de exploração inicial\n",
        "epsilon_decay = 0.995  # Decaimento da taxa de exploração\n",
        "epsilon_min = 0.01    # Valor mínimo de epsilon\n",
        "num_episodes = 10000  # Número total de episódios para treinamento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "THyMNFAnel9l"
      },
      "outputs": [],
      "source": [
        "# Função para escolher ação (política epsilon-gulosa)\n",
        "def choose_action(state):\n",
        "    if np.random.random() < epsilon:\n",
        "        # Exploração: escolhe uma ação aleatória\n",
        "        return np.random.choice(action_space_size)\n",
        "    else:\n",
        "        # Exploração: escolhe a ação com maior valor Q no estado atual\n",
        "        return np.argmax(Q_table[state])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F4vUlXbmel9m",
        "outputId": "6a565b05-93c9-467a-f2ae-07993fd6c8cf"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/esthernunes/.local/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
            "  if not isinstance(terminated, (bool, np.bool8)):\n"
          ]
        }
      ],
      "source": [
        "# Loop de treinamento\n",
        "rewards = []  # Lista para armazenar as recompensas de cada episódio\n",
        "\n",
        "for episode in range(num_episodes):\n",
        "    # Reinicialização do ambiente e discretização do estado inicial\n",
        "    state_raw = env.reset()  # Ajuste para se adaptar à versão do Gym\n",
        "    if isinstance(state_raw, tuple):\n",
        "        state_raw = state_raw[0]  # Pegue apenas o estado, se o retorno for uma tupla\n",
        "    state = discretize_state(state_raw)\n",
        "    done = False\n",
        "    total_reward = 0\n",
        "\n",
        "    while not done:\n",
        "        # Escolher ação com base na política epsilon-gulosa\n",
        "        action = choose_action(state)\n",
        "\n",
        "        # Executar a ação e obter o próximo estado, recompensa e status\n",
        "        step_result = env.step(action)\n",
        "\n",
        "        # Ajuste para lidar com diferentes versões do Gym que retornam tupla com mais de 4 elementos\n",
        "        if len(step_result) == 4:\n",
        "            next_state_raw, reward, done, _ = step_result\n",
        "        elif len(step_result) == 5:  # Para versões mais novas que retornam (state, reward, done, truncated, info)\n",
        "            next_state_raw, reward, done, truncated, _ = step_result\n",
        "            done = done or truncated\n",
        "        else:\n",
        "            raise ValueError(\"O retorno do step tem um número inesperado de elementos.\")\n",
        "\n",
        "        next_state = discretize_state(next_state_raw)\n",
        "\n",
        "        # Escolher a melhor ação futura (maximiza Q)\n",
        "        best_next_action = np.argmax(Q_table[next_state])\n",
        "\n",
        "        # Atualizar a tabela Q usando a Equação de Bellman\n",
        "        td_target = reward + gamma * Q_table[next_state][best_next_action]\n",
        "        td_delta = td_target - Q_table[state][action]\n",
        "        Q_table[state][action] += alpha * td_delta\n",
        "\n",
        "        # Atualizar o estado atual\n",
        "        state = next_state\n",
        "        total_reward += reward\n",
        "\n",
        "    # Armazenar a recompensa total do episódio\n",
        "    rewards.append(total_reward)\n",
        "\n",
        "    # Decaimento da taxa de exploração epsilon\n",
        "    if epsilon > epsilon_min:\n",
        "        epsilon *= epsilon_decay\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p1OMbYdlel9o"
      },
      "outputs": [],
      "source": [
        "# Avaliação do desempenho do agente\n",
        "plt.plot(range(num_episodes), rewards)\n",
        "plt.xlabel('Episódio')\n",
        "plt.ylabel('Recompensa Total')\n",
        "plt.title('Treinamento do Agente CartPole - Q-learning')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lMdrIFtDel9q"
      },
      "outputs": [],
      "source": [
        "# Demonstração do comportamento do agente treinado\n",
        "state_raw = env.reset()  # Ajuste para se adaptar à versão do Gym\n",
        "if isinstance(state_raw, tuple):\n",
        "    state_raw = state_raw[0]  # Pegue apenas o estado, se o retorno for uma tupla\n",
        "state = discretize_state(state_raw)\n",
        "done = False\n",
        "\n",
        "while not done:\n",
        "    # Escolher a ação com maior valor Q no estado atual\n",
        "    action = np.argmax(Q_table[state])\n",
        "\n",
        "    # Executar a ação no ambiente\n",
        "    step_result = env.step(action)\n",
        "\n",
        "    # Ajuste para lidar com diferentes versões do Gym\n",
        "    if len(step_result) == 4:\n",
        "        state_raw, _, done, _ = step_result\n",
        "    elif len(step_result) == 5:  # Para versões mais novas que retornam (state, reward, done, truncated, info)\n",
        "        state_raw, _, done, truncated, _ = step_result\n",
        "        done = done or truncated\n",
        "    else:\n",
        "        raise ValueError(\"O retorno do step tem um número inesperado de elementos.\")\n",
        "\n",
        "    state = discretize_state(state_raw)\n",
        "    env.render()  # Renderiza o ambiente para visualização\n",
        "\n",
        "env.close()  # Fecha o ambiente após a demonstração\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}